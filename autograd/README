# Autograd

Deep learning compiler that implements a Torch-like API and targets CUDA, TT, and
ONNX. Supports both training and inference (see here for whisper, YOLO, SD, and LLaMA)
Formalizations of optimizers can be found in README.tex

### References
To understand two lines of Pytorch:

```
optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)
optimizer.step()
```

**Convex Optimization**
- Rockafellar
- Boyd, Vanderberghe
- Nesterov
- Bubeck
- Recht, Wright

**Non-convex Optimization**
- Farina
- Aggarwal
- Peyr√©
- Strang

**Online Optimization**
- Hazan
- Shwartz

**Parallel Processing**
- Hwu, Kirk, Hajj
- Hennesey, Patterson