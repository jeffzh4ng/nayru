# Learning

Software 1.0 is the act of building deduction machines (Curry-Howard). Software
2.0 is the act of building induction machines, engineering machines to go
from observations to general rules in order to predict unknown properties from
known properties. The probabilistic model of supervised learning is to find a
function h: X -> Y which minimizes the expected loss l: (y,y_hat) -> R of
unaccessible data generating distribution D (curly?) by minimizing the empirical
loss of accessible sample D. Since we only have access to a sample of the
underlying distribution, the loss is only a function of outputs rather than both
inputs and outputs. When Y ⊆ R, we call it regression. When Y ⊆ K ⊆ N, we call it
classification.

**Contents**
1. Models
2. Optimization

**Models**
The model h is selected from a hypothesis class H. Model selection is an overloaded
term, which trifurcates into three parts: 1. hypothesis space selection,
2. hyperparameter slection, and 3. parameter selection. For example, selecting
polynomials, polynomials of degree 2, and polynomials of degree 2 with coeffcients
8 9 10 (8x^2 + 9x + 10). Optimal parameters are tuned against the training set,
while optimal hyperparameters are tuned against validation sets. Overall
performanced is evaluated against test sets.

Hypothesis classes
- linear
- polynomial (taylor)
- non-parametric (knn/svm)
- sinusoidal (fourier)
- piecewise linear (networks)
- probabilistic (MLE, naive bayes)
- turing machines --> undecidable

Underfitting/overfitting (regularization..)
- Expressiveness-complexity tradeoff
    - Min (error + regularization)
        - Error: underfitting
        - regularization: overfitting

        - Lasso regression =  regression error + L1 norm
        - Ridge regression = regression error + L2 norm

**Optimization**
Optimization
The loss function l is ...
optimization...
autograd...
more details can be found in nayru...
 - L0-1: 0 { if y = y_h then 0 else 1 }
        - L1: |y-yh|
        - L2: (y-yh)^2